tmpWidth <- as.integer(args[5])
tmpSize <- as.integer(args[6])
userInput[1,]$id <- as.integer(1)
userInput[1,]$latitude <- as.integer(tmpLatitude)
userInput[1,]$longitude <- as.integer(tmpLongitude)
userInput[1,]$width <- as.integer(tmpWidth)
userInput[1,]$height <- as.integer(tmpHeight)
userInput[1,]$size <- as.integer(tmpSize)
userInput$Continent = as.factor(userInput$Continent)
userInput$proportions = as.factor(userInput$proportions)
# Dodawanie leveli dla factorów
levels(userInput$Continent) <- c("North America", "South America", "Middle America", "Asia", "Europe", "Africa", "Ocean", "Australia")
levels(userInput$proportions) <- c("< 4:3", "4:3", "3:2", "> 3:2")
# Policz dodatkowe kolumny dla rekordu użytkownika
#userInput$Continent <- 'Ocean'
#userInput$Continent <- 'Europe'
userInput$Continent[userInput$longitude > -80 & userInput$longitude < -35 & userInput$latitude > -55 & userInput$latitude < 12] <- 'South America'
userInput$Continent[userInput$longitude > -115 & userInput$longitude < -60 & userInput$latitude > 12 & userInput$latitude < 30] <- 'Middle America'
userInput$Continent[userInput$longitude > -125 & userInput$longitude < -53 & userInput$latitude > 30 & userInput$latitude < 80] <- 'North America'
userInput$Continent[userInput$longitude > -168 & userInput$longitude < -125 & userInput$latitude > 50 & userInput$latitude < 72] <- 'North America'
userInput$Continent[userInput$longitude >  113 & userInput$longitude < 168 & userInput$latitude > -48 & userInput$latitude < -12] <- 'Australia'
userInput$Continent[userInput$longitude > -10 & userInput$longitude < 40 & userInput$latitude > 36 & userInput$latitude < 72] <- 'Europe'
userInput$Continent[userInput$longitude > -18 & userInput$longitude < 35 & userInput$latitude > 5 & userInput$latitude < 36] <- 'Africa'
userInput$Continent[userInput$longitude > 50 & userInput$longitude < 10 & userInput$latitude > -35 & userInput$latitude < 5] <- 'Africa'
userInput$Continent[userInput$longitude > 35 & userInput$longitude < -50 & userInput$latitude > 13 & userInput$latitude < 0] <- 'Africa'
userInput$Continent[userInput$longitude > 40 & userInput$longitude < 145 & userInput$latitude > 30 & userInput$latitude < 85] <- 'Asia'
userInput$Continent[userInput$longitude > 35 & userInput$longitude < 125 & userInput$latitude > 10 & userInput$latitude < 40] <- 'Asia'
userInput$Continent[userInput$longitude > 30 & userInput$longitude < 85 & userInput$latitude > -10 & userInput$latitude < 20] <- 'Asia'
userInput$Continent[userInput$longitude > 140 & userInput$longitude < 180 & userInput$latitude > 50 & userInput$latitude < 75] <- 'Asia'
userInput$Continent[is.na(userInput$Continent)] <- 'Ocean'
if(is.na(userInput$Continent)) {
userInput$Continent <- 'Ocean'
}
userInput$proportions[(userInput$width / userInput$height) < 1.33] <- '< 4:3'
userInput$proportions[(userInput$width / userInput$height) >= 1.33] <- '4:3'
userInput$proportions[(userInput$width / userInput$height) >= 1.45] <- '3:2'
userInput$proportions[(userInput$width / userInput$height) >= 1.55] <- '> 3:2'
userInput$resolution <- as.integer(sqrt(userInput$height^2 + userInput$width^2))
# Tylko do sprawdzania, czy dobrze przekazano parametry
sink(file = "output.txt")
for (i in 1:length(args))
{
cat(args[i])
cat('\n')
cat(userInput$Continent)
}
sink()
# Miejscowy random
set.seed(1)
# Stwórz las drzew decyzyjnych
forest2 <- randomForest(as.factor(good) ~ Continent + proportions + resolution + size + latitude + longitude + width + height, data = train, mtry = 3, importance=TRUE, ntree= treeNumber)
# Dokonaj predykcji
random_forest_result3 <- predict(forest2, userInput, OOB = TRUE, type = "response")
# Zapisz wyniki do pliku
submit3 <- data.frame(Id = userInput$id, good = random_forest_result3)
write.csv(submit3, file = "CSV/userInput.csv", row.names = FALSE)
# Ustawia zmienną, do której może sobie sięgnąć r.net
userResult <- as.integer(submit3[1,2])
userResult <- userResult - 1
tree1 <- rpart(good ~ Continent + resolution + size, data=train, method = "class", control=rpart.control(minsplit=1000, cp=0,99))
fancyRpartPlot(tree1)
library(rpart)
fancyRpartPlot(tree1)
library(rattle)
library(rpart.plot)
library(RColorBrewer)
tree1 <- rpart(good ~ Continent + resolution + size, data=train, method = "class", control=rpart.control(minsplit=1000, cp=0,99))
fancyRpartPlot(tree1)
tree1 <- rpart(good ~ Continent, data=train, method = "class", control=rpart.control(minsplit=1000, cp=0,99))
fancyRpartPlot(tree1)
tree1 <- rpart(good ~ Continent, data=train, method = "anova", control=rpart.control(minsplit=1000, cp=0,99))
fancyRpartPlot(tree1)
forest2 <- randomForest(as.factor(good) ~ Continent + proportions + resolution + size + latitude + longitude + width + height, data = train, mtry = 3, importance=TRUE, ntree= treeNumber)
prop.table(table(train$good))
forest2 <- randomForest(as.factor(good) ~  size + latitude + longitude + width + height, data = train, mtry = 3, importance=TRUE, ntree= treeNumber)
forest2 <- randomForest(as.factor(good) ~ Continent + resolution + size + latitude + longitude + width + height + proportions, data = train, mtry = 3, importance=TRUE, ntree= 100)
# Wypisanie ważności poszczególnych parametrów podczas podejmowania decyzji
importance(forest2)
#varImpPlot(forest2)
# Dokonaj analizy danych na podstawie lasu
random_forest_result2 <- predict(forest2, test, OOB = TRUE, type = "response")
# Stwórz plik wynikowy
submit2 <- data.frame(Id = test$id, good = random_forest_result2)
write.csv(submit2, file = "CSV/random_forest2.csv", row.names = FALSE)
prop.table(table(submit2$good))
View(userInput)
View(submit3)
View(submit3)
userResult <- as.integer(submit3[1,2])
userResult <- userResult - 1
View(userInput)
library(neuralnet)
norm.fun = function(x){(x - min(x))/(max(x) - min(x))}
train.norm = apply(train[,2:6], 2, norm.fun)
neural <- neuralnet(good ~ longitude + latitude + width + height + size, data=train, hidden = 5, stepmax = 1000000, threshold = 0.01, linear.output = FALSE)
library("tm")
install.packages('tm')
library("tm")
data("crude")
revs <- tm_map(crude, content_transformer(tolower))
revs <- tm_map(revs, removeWords, stopwords("english"))
revs <- tm_map(revs, removePunctuation)
revs <- tm_map(revs, removeNumbers)
revs <- tm_map(revs, stripWhitespace)
dtm <- DocumentTermMatrix(revs)
revs <- train$name
revs <- tm_map(crude, content_transformer(tolower))
revs <- tm_map(revs, removeWords, stopwords("english"))
revs <- tm_map(revs, removePunctuation)
revs <- tm_map(revs, removeNumbers)
revs <- tm_map(revs, stripWhitespace)
dtm <- DocumentTermMatrix(revs)
train$nameLength <- sapply(gregexpr("\\W+", train$name), length) + 1
train$nameLength <- sapply(gregexpr("\\W+", train$name), length)
train$nameLength <- sapply(gregexpr("\\W+", train$name), length) + 1
train$nameLength <- sapply(gregexpr("\\W+", train$name), length) + 1
?gregexpr
train$nameLength <- sapply(grep("\\W+", train$name), length) + 1
train$nameLength <- sapply(grepl("\\W+", train$name), length) + 1
train$nameLength <- sapply(grepl(" ", train$name), length) + 1
train$nameLength <- sapply(grepexpr(" ", train$name), length) + 1
train$nameLength <- sapply(gregexpr(" ", train$name), length) + 1
train$nameLength <- sapply(gregexpr("", train$name), length) + 1
train$nameLength <- sapply(gregexpr("8", train$name), length) + 1
train$nameLength <- sapply(gregexpr("8", train$name), length)
train$nameLength <- sapply(gregexpr(" ", train$name), length)
train$nameLength <- sapply(gregexpr("[A-z]\\W+", train$name), length) + 1L
train$nameLength <- sapply(gregexpr("\\W+", train$name), function(x) sum(x>0) ) + 1
train$nameLength <- sapply(gregexpr("\\W+", train$name), function(x) sum(x>0) ) + 1
train$nameLength[length(train$name)  <= 0] <- train$nameLength - 1
train$nameLength[length(train$name)  <= 1] <- train$nameLength - 1
train$nameLength[length(train$name)  <= 2] <- train$nameLength - 1
train$nameLength[length(train$name) == 0] <- train$nameLength - 1
train$nameLength[length(train$name) > 0] <- train$nameLength - 1
train$nameLength[length(train$name) > 0] <- train$nameLength + 1
train$nameLength[length(train$name) < 1] <- train$nameLength + 1
train$nameLength[length(train$name) < 2] <- train$nameLength - 1
train$nameLength[length(train$name) < 3] <- train$nameLength - 1
train$nameLength[length(train$name) < 4] <- train$nameLength - 1
train$nameLength[length(train$name) < 6] <- train$nameLength - 1
length(train$name)
length(train$name[])
table(length(train$name))
table(train$nameLength)
table(length(train$name)
table(length(train$name))
table(lenght(train$name))
table(length(train$name))
table(length(train$name[]))
table(length(train[]$name))
length("")
length("1")
length("12")
length("123")
length("123", "2")
length("123 edsa",)
length("123 edsa")
train$nameLength[nchar(train$name) < 1] <- train$nameLength - 1
nchar(train$name)
train$nameLength[nchar(train$name) < 1] <- 0
test$nameLength <- sapply(gregexpr("\\W+", test$name), function(x) sum(x>0) ) + 1
test$nameLength[nchar(test$name) < 1] <- 0
tree1 <- rpart(good ~ Continent + resolution + size + nameLength, data=train, method = "class", control=rpart.control(minsplit=1000, cp=0,99))
library(rpart)
tree1 <- rpart(good ~ Continent + resolution + size + nameLength, data=train, method = "class", control=rpart.control(minsplit=1000, cp=0,99))
fancyRpartPlot(tree1)
library(rpart.plot)
fancyRpartPlot(tree1)
library(rattle)
library(rpart.plot)
library(RColorBrewer)
fancyRpartPlot(tree1)
prop.table(table(train$nameLength, train$good), 1)
tree1 <- rpart(good ~ nameLength, data=train, method = "class", control=rpart.control(minsplit=1000, cp=0,99))
fancyRpartPlot(tree1)
tree1 <- rpart(good ~ nameLength, data=train, method = "class", control=rpart.control(minsplit=3000, cp=0,99))
fancyRpartPlot(tree1)
tree1 <- rpart(good ~ nameLength, data=train, method = "class", control=rpart.control(minsplit=300, cp=0,99))
fancyRpartPlot(tree1)
tree1 <- rpart(good ~ nameLength, data=train, method = "class", control=rpart.control(minsplit=5000, cp=0,99))
fancyRpartPlot(tree1)
tree1 <- rpart(good ~ nameLength, data=train, method = "class", control=rpart.control(minsplit=10000, cp=0,99))
fancyRpartPlot(tree1)
tree1 <- rpart(good ~ nameLength, data=train, method = "class", control=rpart.control(minsplit=20000, cp=0,99))
fancyRpartPlot(tree1)
tree1 <- rpart(good ~ nameLength, data=train, method = "class", control=rpart.control(minsplit=200, cp=0,99))
fancyRpartPlot(tree1)
tree1 <- rpart(good ~ nameLength, data=train, method = "class", control=rpart.control(minsplit=20, cp=0,99))
fancyRpartPlot(tree1)
tree1 <- rpart(good ~ nameLength, data=train, method = "class", control=rpart.control(minsplit=2, cp=0,99))
fancyRpartPlot(tree1)
tree1 <- rpart(good ~ nameLength, data=train, method = "class", control=rpart.control(minsplit=200, cp=0,5))
fancyRpartPlot(tree1)
tree1 <- rpart(good ~ nameLength, data=train, method = "class", control=rpart.control(minsplit=20, cp=0,5))
fancyRpartPlot(tree1)
tree1 <- rpart(good ~ nameLength, data=train, method = "class", control=rpart.control(minsplit=2000, cp=0,5))
fancyRpartPlot(tree1)
tree1 <- rpart(good ~ nameLength, data=train, method = "class", control=rpart.control(minsplit=2000, cp=0,5))
fancyRpartPlot(tree1)
tree1 <- rpart(good ~ nameLength, data=train, method = "anova", control=rpart.control(minsplit=2000, cp=0,5))
fancyRpartPlot(tree1)
tree1 <- rpart(good ~ nameLength, data=train, method = "class", control=rpart.control(minsplit=2000, cp=0,5))
fancyRpartPlot(tree1)
library(randomForest)
set.seed(4)
# Stwórz las drzew decyzyjnych
forest2 <- randomForest(as.factor(good) ~ Continent + resolution + size + latitude + longitude + width + height + proportions + nameLength, data = train, mtry = 3, importance=TRUE, ntree= 100)
# Wypisanie ważności poszczególnych parametrów podczas podejmowania decyzji
#importance(forest2)
varImpPlot(forest2)
# Dokonaj analizy danych na podstawie lasu
random_forest_result2 <- predict(forest2, test, OOB = TRUE, type = "response")
# Stwórz plik wynikowy
submit2 <- data.frame(Id = test$id, good = random_forest_result2)
write.csv(submit2, file = "CSV/random_forest2.csv", row.names = FALSE)
prop.table(table(submit2$good))
train$captionLength <- sapply(gregexpr("\\W+", train$caption), function(x) sum(x>0) ) + 1
train$captionLength[nchar(train$caption) < 1] <- 0
test$captionLength <- sapply(gregexpr("\\W+", test$caption), function(x) sum(x>0) ) + 1
test$captionLength[nchar(test$caption) < 1] <- 0
prop.table(table(train$captionLength, train$good), 1)
tree1 <- rpart(good ~ nameLength, data=train, method = "class", control=rpart.control(minsplit=2000, cp=0,5))
tree1 <- rpart(good ~ captionLength, data=train, method = "class", control=rpart.control(minsplit=2000, cp=0,5))
fancyRpartPlot(tree1)
forest2 <- randomForest(as.factor(good) ~ Continent + resolution + size + latitude + longitude + width + height + proportions + nameLength + captionLength, data = train, mtry = 3, importance=TRUE, ntree= 100)
# Wypisanie ważności poszczególnych parametrów podczas podejmowania decyzji
#importance(forest2)
varImpPlot(forest2)
# Dokonaj analizy danych na podstawie lasu
random_forest_result2 <- predict(forest2, test, OOB = TRUE, type = "response")
# Stwórz plik wynikowy
submit2 <- data.frame(Id = test$id, good = random_forest_result2)
write.csv(submit2, file = "CSV/random_forest2.csv", row.names = FALSE)
prop.table(table(submit2$good))
train$descriptionLength <- sapply(gregexpr("\\W+", train$description), function(x) sum(x>0) ) + 1
train$descriptionLength[nchar(train$description) < 1] <- 0
test$descriptionLength <- sapply(gregexpr("\\W+", test$description), function(x) sum(x>0) ) + 1
test$descriptionLength[nchar(test$description) < 1] <- 0
prop.table(table(train$descriptionLength, train$good), 1)
tree1 <- rpart(good ~ decriptionLength, data=train, method = "class", control=rpart.control(minsplit=2000, cp=0,5))
fancyRpartPlot(tree1)
train$descriptionLength <- sapply(gregexpr("\\W+", train$description), function(x) sum(x>0) ) + 1
train$descriptionLength[nchar(train$description) < 1] <- 0
test$descriptionLength <- sapply(gregexpr("\\W+", test$description), function(x) sum(x>0) ) + 1
test$descriptionLength[nchar(test$description) < 1] <- 0
tree1 <- rpart(good ~ decriptionLength, data=train, method = "class", control=rpart.control(minsplit=2000, cp=0,5))
tree1 <- rpart(good ~ descriptionLength, data=train, method = "class", control=rpart.control(minsplit=2000, cp=0,5))
fancyRpartPlot(tree1)
tree1 <- rpart(good ~ descriptionLength, data=train, method = "class", control=rpart.control(minsplit=5000, cp=0,5))
fancyRpartPlot(tree1)
tree1 <- rpart(good ~ descriptionLength, data=train, method = "class", control=rpart.control(minsplit=500, cp=0,5))
fancyRpartPlot(tree1)
tree1 <- rpart(good ~ descriptionLength, data=train, method = "class", control=rpart.control(minsplit=300, cp=0,5))
fancyRpartPlot(tree1)
tree1 <- rpart(good ~ descriptionLength, data=train, method = "class", control=rpart.control(minsplit=300, cp=0,1))
fancyRpartPlot(tree1)
forest2 <- randomForest(as.factor(good) ~ Continent + resolution + size + latitude + longitude + width + height + proportions + nameLength + captionLength + descriptionLength, data = train, mtry = 3, importance=TRUE, ntree= 100)
varImpPlot(forest2)
# Dokonaj analizy danych na podstawie lasu
random_forest_result2 <- predict(forest2, test, OOB = TRUE, type = "response")
# Stwórz plik wynikowy
submit2 <- data.frame(Id = test$id, good = random_forest_result2)
write.csv(submit2, file = "CSV/random_forest2.csv", row.names = FALSE)
prop.table(table(submit2$good))
forest2 <- randomForest(as.factor(good) ~ Continent + resolution + size + latitude + longitude + width + height + proportions + nameLength + captionLength + descriptionLength, data = train, mtry = 5, importance=TRUE, ntree= 100)
# Wypisanie ważności poszczególnych parametrów podczas podejmowania decyzji
#importance(forest2)
varImpPlot(forest2)
# Dokonaj analizy danych na podstawie lasu
random_forest_result2 <- predict(forest2, test, OOB = TRUE, type = "response")
# Stwórz plik wynikowy
submit2 <- data.frame(Id = test$id, good = random_forest_result2)
write.csv(submit2, file = "CSV/random_forest2.csv", row.names = FALSE)
prop.table(table(submit2$good))
forest2 <- randomForest(as.factor(good) ~ Continent + resolution + size + latitude + longitude + width + height + proportions + nameLength + captionLength + descriptionLength, data = train, mtry = 11, importance=TRUE, ntree= 100)
# Wypisanie ważności poszczególnych parametrów podczas podejmowania decyzji
#importance(forest2)
varImpPlot(forest2)
# Dokonaj analizy danych na podstawie lasu
random_forest_result2 <- predict(forest2, test, OOB = TRUE, type = "response")
# Stwórz plik wynikowy
submit2 <- data.frame(Id = test$id, good = random_forest_result2)
write.csv(submit2, file = "CSV/random_forest2.csv", row.names = FALSE)
prop.table(table(submit2$good))
forest1 <- cforest(as.factor(good) ~ Continent + resolution + size + latitude + longitude + width + height + proportions + nameLength + captionLength + descriptionLength, data=train, controls = cforest_unbiased(ntree = 10, mtry = 11))
random_forest_result <- predict(forest1, test, OOB = TRUE, type = "response")
submit <- data.frame(Id = test$id, good = random_forest_result)
write.csv(submit, file = "CSV/random_forest.csv", row.names = FALSE)
prop.table(table(submit$good))
library(party)
set.seed(1)
set.seed(1)
forest1 <- cforest(as.factor(good) ~ Continent + resolution + size + latitude + longitude + width + height + proportions + nameLength + captionLength + descriptionLength, data=train, controls = cforest_unbiased(ntree = 10, mtry = 11))
random_forest_result <- predict(forest1, test, OOB = TRUE, type = "response")
submit <- data.frame(Id = test$id, good = random_forest_result)
write.csv(submit, file = "CSV/random_forest.csv", row.names = FALSE)
prop.table(table(submit$good))
forest2 <- randomForest(as.factor(good) ~ Continent + resolution + size + latitude + longitude + width + height + proportions + nameLength + captionLength + descriptionLength, data = train, mtry = 11, importance=TRUE, ntree= 500)
# Wypisanie ważności poszczególnych parametrów podczas podejmowania decyzji
#importance(forest2)
varImpPlot(forest2)
# Dokonaj analizy danych na podstawie lasu
random_forest_result2 <- predict(forest2, test, OOB = TRUE, type = "response")
# Stwórz plik wynikowy
submit2 <- data.frame(Id = test$id, good = random_forest_result2)
write.csv(submit2, file = "CSV/random_forest2.csv", row.names = FALSE)
prop.table(table(submit2$good))
train <- read.csv("CSV/training.csv", stringsAsFactors = FALSE)
test <- read.csv("CSV/test.csv", stringsAsFactors = FALSE)
source('ExtraColumns.R')
library(party)
set.seed(1)
forest1 <- cforest(as.factor(good) ~ Continent + resolution + size + latitude + longitude + width + height + proportions + nameLength + captionLength + descriptionLength, data=train, controls = cforest_unbiased(ntree = 10, mtry = 11))
random_forest_result <- predict(forest1, test, OOB = TRUE, type = "response")
submit <- data.frame(Id = test$id, good = random_forest_result)
write.csv(submit, file = "CSV/random_forest.csv", row.names = FALSE)
prop.table(table(submit$good))
train.sub <- subset(train, train$id > 35000)
test.sub <- subset(test, test$id > 47000)
test.sub <- subset(test, test$id > 50000)
set.seed(1)
forest1 <- cforest(as.factor(good) ~ Continent + resolution + size + latitude + longitude + width + height + proportions + nameLength + captionLength + descriptionLength, data=train.sub, controls = cforest_unbiased(ntree = 10, mtry = 11))
random_forest_result <- predict(forest1, test.sub, OOB = TRUE, type = "response")
submit <- data.frame(Id = test$id, good = random_forest_result)
write.csv(submit, file = "CSV/random_forest.csv", row.names = FALSE)
prop.table(table(submit$good))
submit <- data.frame(Id = test.sub$id, good = random_forest_result)
write.csv(submit, file = "CSV/random_forest.csv", row.names = FALSE)
prop.table(table(submit$good))
source('setup')
source('setup.R')
forest1 <- cforest(as.factor(good) ~ Continent + resolution + size + latitude + longitude + width + height + proportions + nameLength + captionLength + descriptionLength, data=train.sub, controls = cforest_unbiased(ntree = 10, mtry = 11))
random_forest_result <- predict(forest1, test.sub, OOB = TRUE, type = "response")
forest1 <- cforest(as.factor(good) ~ Continent + resolution + size + latitude + longitude + width + height + proportions + nameLength + captionLength + descriptionLength, data=train, controls = cforest_unbiased(ntree = 10, mtry = 11))
random_forest_result <- predict(forest1, test.sub, OOB = TRUE, type = "response")
random_forest_result <- predict(forest1, test, OOB = TRUE, type = "response")
submit <- data.frame(Id = test$id, good = random_forest_result)
source('ExtraColumns.R')
train.sub <- subset(train, train$id > 35000)
test.sub <- subset(test, test$id > 50000)
set.seed(1)
forest1 <- cforest(as.factor(good) ~ Continent + resolution + size + latitude + longitude + width + height + proportions + nameLength + captionLength + descriptionLength, data=train, controls = cforest_unbiased(ntree = 10, mtry = 11))
random_forest_result <- predict(forest1, test, OOB = TRUE, type = "response")
submit <- data.frame(Id = test$id, good = random_forest_result)
write.csv(submit, file = "CSV/random_forest.csv", row.names = FALSE)
prop.table(table(submit$good))
forest2 <- randomForest(as.factor(good) ~ Continent + resolution + size + latitude + longitude + width + height + proportions + nameLength + captionLength + descriptionLength, data = train, mtry = 11, importance=TRUE, ntree= 100)
# Wypisanie ważności poszczególnych parametrów podczas podejmowania decyzji
#importance(forest2)
#varImpPlot(forest2)
# Dokonaj analizy danych na podstawie lasu
random_forest_result2 <- predict(forest2, test, OOB = TRUE, type = "response")
# Stwórz plik wynikowy
submit2 <- data.frame(Id = test$id, good = random_forest_result2)
write.csv(submit2, file = "CSV/random_forest2.csv", row.names = FALSE)
prop.table(table(submit2$good))
train.sub <- subset(train, train$id > 38000)
test.sub <- subset(test, test$id > 50000)
set.seed(1)
forest1 <- cforest(as.factor(good) ~ Continent + resolution + size + latitude + longitude + width + height + proportions + nameLength + captionLength + descriptionLength, data=train, controls = cforest_unbiased(ntree = 10, mtry = 11))
library(party)
forest1 <- cforest(as.factor(good) ~ Continent + resolution + size + latitude + longitude + width + height + proportions + nameLength + captionLength + descriptionLength, data=train, controls = cforest_unbiased(ntree = 10, mtry = 11))
random_forest_result <- predict(forest1, test.sub, OOB = TRUE, type = "response")
submit <- data.frame(Id = test.sub$id, good = random_forest_result)
write.csv(submit, file = "CSV/random_forest.csv", row.names = FALSE)
prop.table(table(submit$good))
train.sub <- subset(train, train$id > 30000)
train.sub <- subset(train, train$id > 30000)
test.sub <- subset(test, test$id > 50000)
set.seed(1)
forest1 <- cforest(as.factor(good) ~ Continent + resolution + size + latitude + longitude + width + height + proportions + nameLength + captionLength + descriptionLength, data=train.sub, controls = cforest_unbiased(ntree = 10, mtry = 11))
random_forest_result <- predict(forest1, test, OOB = TRUE, type = "response")
submit <- data.frame(Id = test$id, good = random_forest_result)
write.csv(submit, file = "CSV/random_forest.csv", row.names = FALSE)
prop.table(table(submit$good))
install.packages(nnet)
install.packages("nnet")
library(nnet)
neuralNetworkModel <- nnet(trainingInput, trainingOutput, size = 19, rang = 0.1, decay = 5e-4, maxit = 2000)
neuralNetworkModel <- nnet(train, test, size = 19, rang = 0.1, decay = 5e-4, maxit = 2000)
neuralNetworkModel <- nnet(train, test, size = 19, rang = 0.1, decay = 5e-4, maxit = 2000)
comb <- rbind(train, test)
ir.nn2 <- nnet(good ~ longitude + latitude + width + height + size, data = combi, subset = train, size = 2, rang = 0.1, decay = 5e-4, maxit = 200)
ir.nn2 <- nnet(good ~ longitude + latitude + width + height + size, data = comb, subset = train, size = 2, rang = 0.1, decay = 5e-4, maxit = 200)
ir.nn2 <- nnet(good ~ longitude + latitude + width + height + size, data = comb, subset = comb$id < 40000, size = 2, rang = 0.1, decay = 5e-4, maxit = 200)
submit4 <-  data.frame(Id = test$id, neuralNetworkTest)
submit4 <-  data.frame(Id = test$id, ir.nn2)
comb(ird$species[-train], predict(ir.nn2, comb[-train,], type = "class"))
comb(comb$good[-train], predict(ir.nn2, comb[-train,], type = "class"))
submit4 <-  comb(comb$good[-train], predict(ir.nn2, comb[-train,], type = "class"))
table(comb$good[-train], predict(ir.nn2, comb[-train,], type = "class"))
table(comb$good[comb$id > 40000], predict(ir.nn2, comb[-train,], type = "class"))
table(comb[-train], predict(ir.nn2, comb[-train,], type = "class"))
table(comb, predict(ir.nn2, comb[-train,], type = "class"))
ir.nn2 <- nnet(good ~ longitude + latitude + width + height + size, data = comb, subset = comb$id < 40000, size = 2, rang = 0.1, decay = 5e-4, maxit = 200)
table(comb$good[-train], )
table(, predict(ir.nn2, comb[-train,], type = "class"))
comb$good[-train]
View(comb)
table(comb$good, predict(ir.nn2, comb[-train,], type = "class"))
table(comb$good[-train])
table(comb$good)
table(train$good)
table(comb$good)
table(comb$good[-train])
table(comb$good[-comb$id < 40000])
table(comb$good[comb$id < 40000])
table(comb$good[comb$id > 40000])
table(comb$good[comb$id > 40000], predict(ir.nn2, comb[-train,], type = "class"))
table(comb$good[comb$id > 40000], predict(ir.nn2, comb[-train,], type = "class"), 1)
table(comb$good[comb$id > 40000], predict(ir.nn2, comb[-train,], type = "class"))
predict(ir.nn2, comb[-train,], type = "class")
predict(ir.nn2, comb[comb$id > 40000], type = "class")
table(comb$good[comb$id > 40000], predict(ir.nn2, comb[comb$id > 40000], type = "class"))
ir.nn2 <- nnet(good ~ longitude + latitude + width + height + size, data = comb, subset = comb$id < 40000, size = 2, rang = 0.1, decay = 5e-4, maxit = 200)
require(quantmod) #for Lag()
install.packages(quantmod)
install.packages("quantmod")
install.packages("caret")
model <- train(good ~ longitude + latitude + width + height + size, comb, method='nnet', linout=TRUE, trace = FALSE,
#Grid of tuning parameters to try:
tuneGrid=expand.grid(.size=c(1,5,10),.decay=c(0,0.001,0.1)))
library(randomForest)
#commandArgs <- function() c(10, -90, -180, 480, 720, 23)
source('ExtraColumns.R')
source('LoadUserRecord.R')
commandArgs <- function() c(10, -90, -180, 480, 720, 23)
source('LoadUserRecord.R')
# Tylko do sprawdzania, czy dobrze przekazano parametry
sink(file = "output.txt")
for (i in 1:length(args))
cat(args[i])
{
cat('\n')
}
sink()
# Miejscowy random
set.seed(1)
# Stwórz las drzew decyzyjnych
forest2 <- randomForest(as.factor(good) ~ Continent + resolution + size + latitude + longitude + width + height + proportions, data = train, mtry = 3, importance=TRUE, ntree= treeNumber)
# Dokonaj predykcji
random_forest_result3 <- predict(forest2, userInput, OOB = TRUE, type = "response")
# Zapisz wyniki do pliku
submit3 <- data.frame(Id = userInput$id, good = random_forest_result3)
# Ustawia zmienną, do której może sobie sięgnąć r.net
userResult <- as.integer(submit3[1,2])
userResult <- userResult - 1
set.seed(1)
# Stwórz las
forest1 <- cforest(as.factor(good) ~ Continent + resolution + size + latitude + longitude + width + height + proportions, data=train, controls = cforest_unbiased(ntree = 10, mtry = 3))
# Dokonaj predykcji i zapisz wynik
library(party)
forest1 <- cforest(as.factor(good) ~ Continent + resolution + size + latitude + longitude + width + height + proportions, data=train, controls = cforest_unbiased(ntree = 10, mtry = 3))
# Dokonaj predykcji i zapisz wynik
random_forest_result4 <- predict(forest1, userInput, OOB = TRUE, type = "response")
set.seed(5)
# Stwórz las
forest1 <- cforest(as.factor(good) ~ Continent + resolution + size + latitude + longitude + width + height + proportions, data=train, controls = cforest_unbiased(ntree = 10, mtry = 3))
# Dokonaj predykcji i zapisz wynik
random_forest_result4 <- predict(forest1, userInput, OOB = TRUE, type = "response")
forest2 <- randomForest(as.factor(good) ~ Continent + resolution + size + latitude + longitude + width + height + proportions, data = train, mtry = 3, importance=TRUE, ntree= treeNumber)
# Dokonaj predykcji
random_forest_result3 <- predict(forest2, userInput, OOB = TRUE, type = "response")
# Zapisz wyniki do pliku
submit3 <- data.frame(Id = userInput$id, good = random_forest_result3)
# Ustawia zmienną, do której może sobie sięgnąć r.net
userResult <- as.integer(submit3[1,2])
userResult <- userResult - 1
View(submit3)
random_forest_result4 <- predict(forest1, userInput, OOB = TRUE, type = "response")
library(party)
library(randomForest)
commandArgs <- function() c(10, -90, -180, 480, 720, 23)
train <- read.csv("CSV/training.csv", stringsAsFactors = FALSE)
test <- read.csv("CSV/test.csv", stringsAsFactors = FALSE)
source('ExtraColumns.R')
source('LoadUserRecord.R')
sink(file = "output.txt")
for (i in 1:length(args))
{
cat(args[i])
cat('\n')
}
sink()
# Miejscowy random
set.seed(5)
# Stwórz las
forest1 <- cforest(as.factor(good) ~ Continent + resolution + size + latitude + longitude + width + height + proportions, data=train, controls = cforest_unbiased(ntree = 10, mtry = 3))
# Dokonaj predykcji i zapisz wynik
random_forest_result4 <- predict(forest1, userInput, OOB = TRUE, type = "response")
submit4 <- data.frame(Id = userInput$id, good = random_forest_result4)
write.csv(submit4, file = "CSV/userInput.csv", row.names = FALSE)
userResult <- as.integer(submit4[1,2])
userResult <- userResult - 1
levels(train)
levels(train$good)
View(train)
levels(train$Continent)
levels(userInput$Continent)
levels(userInput$proportions)
levels(train$proportions)
source('LoadUserRecord.R')
forest1 <- cforest(as.factor(good) ~ Continent + resolution + size + latitude + longitude + width + height + proportions, data=train, controls = cforest_unbiased(ntree = 10, mtry = 3))
# Dokonaj predykcji i zapisz wynik
random_forest_result4 <- predict(forest1, userInput, OOB = TRUE, type = "response")
submit4 <- data.frame(Id = userInput$id, good = random_forest_result4)
write.csv(submit4, file = "CSV/userInput.csv", row.names = FALSE)
# Ustawia zmienną, do której może sobie sięgnąć r.net
userResult <- as.integer(submit4[1,2])
userResult <- userResult - 1
levels(train$proportions)
View(submit4)
